{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data Exploration","metadata":{}},{"cell_type":"markdown","source":"Let's use the `.info()`, `.describe()`, and `.head()` methods to learn more about the dataset.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv(\"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T20:07:00.945976Z","iopub.execute_input":"2024-04-02T20:07:00.946365Z","iopub.status.idle":"2024-04-02T20:07:02.916921Z","shell.execute_reply.started":"2024-04-02T20:07:00.946334Z","shell.execute_reply":"2024-04-02T20:07:02.916010Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 50000 entries, 0 to 49999\nData columns (total 2 columns):\n #   Column     Non-Null Count  Dtype \n---  ------     --------------  ----- \n 0   review     50000 non-null  object\n 1   sentiment  50000 non-null  object\ndtypes: object(2)\nmemory usage: 781.4+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T20:07:02.918715Z","iopub.execute_input":"2024-04-02T20:07:02.919015Z","iopub.status.idle":"2024-04-02T20:07:03.023281Z","shell.execute_reply.started":"2024-04-02T20:07:02.918982Z","shell.execute_reply":"2024-04-02T20:07:03.022309Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                                                   review sentiment\ncount                                               50000     50000\nunique                                              49582         2\ntop     Loved today's show!!! It was a variety and not...  positive\nfreq                                                    5     25000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>50000</td>\n      <td>50000</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>49582</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>Loved today's show!!! It was a variety and not...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>5</td>\n      <td>25000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T20:07:03.024729Z","iopub.execute_input":"2024-04-02T20:07:03.025376Z","iopub.status.idle":"2024-04-02T20:07:03.034602Z","shell.execute_reply.started":"2024-04-02T20:07:03.025342Z","shell.execute_reply":"2024-04-02T20:07:03.033598Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                              review sentiment\n0  One of the other reviewers has mentioned that ...  positive\n1  A wonderful little production. <br /><br />The...  positive\n2  I thought this was a wonderful way to spend ti...  positive\n3  Basically there's a family where a little boy ...  negative\n4  Petter Mattei's \"Love in the Time of Money\" is...  positive","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Basically there's a family where a little boy ...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"We can see that the dateset contains one column, the review, along with the binary target variable that we'd like to predict, sentiment (positive/negative). There are no missing values (phew!!).","metadata":{}},{"cell_type":"code","source":"df['sentiment'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T20:07:04.385682Z","iopub.execute_input":"2024-04-02T20:07:04.386062Z","iopub.status.idle":"2024-04-02T20:07:04.399486Z","shell.execute_reply.started":"2024-04-02T20:07:04.386031Z","shell.execute_reply":"2024-04-02T20:07:04.398474Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"sentiment\npositive    25000\nnegative    25000\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"The dataset is as balanced as it could be, we have the same number of positive and negative samples. 25000 each.","metadata":{}},{"cell_type":"code","source":"df['review'].head(20)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T20:07:05.665903Z","iopub.execute_input":"2024-04-02T20:07:05.666293Z","iopub.status.idle":"2024-04-02T20:07:05.673745Z","shell.execute_reply.started":"2024-04-02T20:07:05.666264Z","shell.execute_reply":"2024-04-02T20:07:05.672824Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"0     One of the other reviewers has mentioned that ...\n1     A wonderful little production. <br /><br />The...\n2     I thought this was a wonderful way to spend ti...\n3     Basically there's a family where a little boy ...\n4     Petter Mattei's \"Love in the Time of Money\" is...\n5     Probably my all-time favorite movie, a story o...\n6     I sure would like to see a resurrection of a u...\n7     This show was an amazing, fresh & innovative i...\n8     Encouraged by the positive comments about this...\n9     If you like original gut wrenching laughter yo...\n10    Phil the Alien is one of those quirky films wh...\n11    I saw this movie when I was about 12 when it c...\n12    So im not a big fan of Boll's work but then ag...\n13    The cast played Shakespeare.<br /><br />Shakes...\n14    This a fantastic movie of three prisoners who ...\n15    Kind of drawn in by the erotic scenes, only to...\n16    Some films just simply should not be remade. T...\n17    This movie made it into one of my top 10 most ...\n18    I remember this film,it was the first film i h...\n19    An awful film! It must have been up against so...\nName: review, dtype: object"},"metadata":{}}]},{"cell_type":"markdown","source":"A quick look at the first 20 rows of reviews reveals that the dataset contains some HTML elements like the `<br />` line break element.","metadata":{}},{"cell_type":"markdown","source":"## Data Cleaning","metadata":{}},{"cell_type":"code","source":"df['review'].str.contains(r'<.*?>').sum()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T20:07:09.320773Z","iopub.execute_input":"2024-04-02T20:07:09.321509Z","iopub.status.idle":"2024-04-02T20:07:09.385750Z","shell.execute_reply.started":"2024-04-02T20:07:09.321476Z","shell.execute_reply":"2024-04-02T20:07:09.384792Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"29202"},"metadata":{}}]},{"cell_type":"markdown","source":"There are 29200 reviews containing html elements! let's remove them.","metadata":{}},{"cell_type":"code","source":"import re\n\nhtml = re.compile(r'<.*?>')\ndf['review'] = df['review'].str.replace(html, '', regex=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T20:07:11.685310Z","iopub.execute_input":"2024-04-02T20:07:11.685888Z","iopub.status.idle":"2024-04-02T20:07:11.871311Z","shell.execute_reply.started":"2024-04-02T20:07:11.685851Z","shell.execute_reply":"2024-04-02T20:07:11.870440Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Let's also lowecase the text.","metadata":{}},{"cell_type":"code","source":"df['review'] = df['review'].str.lower()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T20:07:18.305923Z","iopub.execute_input":"2024-04-02T20:07:18.306282Z","iopub.status.idle":"2024-04-02T20:07:18.499843Z","shell.execute_reply.started":"2024-04-02T20:07:18.306257Z","shell.execute_reply":"2024-04-02T20:07:18.498836Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Data Visualization","metadata":{}},{"cell_type":"markdown","source":"Let's visualize the dataset using a word cloud.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nimport nltk\nfrom nltk.corpus import stopwords","metadata":{"execution":{"iopub.status.busy":"2024-04-02T20:07:42.555681Z","iopub.execute_input":"2024-04-02T20:07:42.556334Z","iopub.status.idle":"2024-04-02T20:07:43.616844Z","shell.execute_reply.started":"2024-04-02T20:07:42.556303Z","shell.execute_reply":"2024-04-02T20:07:43.616021Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"class CustomCountVectorizer(CountVectorizer):\n    def build_tokenizer(self):\n        tokenizer = super().build_tokenizer()\n        return lambda doc: [token for token in tokenizer(doc) if token not in stopwords.words('english')] # We will ignore stop words\n\ntokenizer = CustomCountVectorizer().build_tokenizer()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T20:07:43.618770Z","iopub.execute_input":"2024-04-02T20:07:43.619180Z","iopub.status.idle":"2024-04-02T20:07:43.625464Z","shell.execute_reply.started":"2024-04-02T20:07:43.619145Z","shell.execute_reply":"2024-04-02T20:07:43.624449Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"markdown","source":"Substitute the values in the target column with 0/1.","metadata":{}},{"cell_type":"code","source":"df['sentiment'].replace({\"positive\": 1, \"negative\": 0}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T20:08:08.945781Z","iopub.execute_input":"2024-04-02T20:08:08.946172Z","iopub.status.idle":"2024-04-02T20:08:08.985977Z","shell.execute_reply.started":"2024-04-02T20:08:08.946144Z","shell.execute_reply":"2024-04-02T20:08:08.985019Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Split the dataset into a training (80%) and a test (20%) set with stratification.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.2, stratify=df['sentiment'], random_state=1337)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T20:08:15.486061Z","iopub.execute_input":"2024-04-02T20:08:15.486740Z","iopub.status.idle":"2024-04-02T20:08:15.519985Z","shell.execute_reply.started":"2024-04-02T20:08:15.486710Z","shell.execute_reply":"2024-04-02T20:08:15.519036Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"We will use a TF-IDF (term frequency–inverse document frequency) vectorizer because it won't give greater importance to tokens that are common among most reviews and thus have little predictive power such as *movie*, *film*, *story*, *one*, ...\n\nAnd we will also remove stop words for the same reason.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# Convert stop words to a set to speed up the stop word lookup\nstopwords = {word for word in stopwords.words('english')}\n\nclass CustomTfidfVectorizer(TfidfVectorizer):\n    def build_tokenizer(self):\n        tokenizer = super().build_tokenizer()\n        return lambda doc: [token for token in tokenizer(doc) if token not in stopwords] # Remove stop words\n\nvectorizer = CustomTfidfVectorizer()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T20:08:45.245625Z","iopub.execute_input":"2024-04-02T20:08:45.246007Z","iopub.status.idle":"2024-04-02T20:08:45.256908Z","shell.execute_reply.started":"2024-04-02T20:08:45.245980Z","shell.execute_reply":"2024-04-02T20:08:45.255955Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"X_train_vectorized = vectorizer.fit_transform(X_train)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T20:08:45.480494Z","iopub.execute_input":"2024-04-02T20:08:45.481331Z","iopub.status.idle":"2024-04-02T20:08:54.408081Z","shell.execute_reply.started":"2024-04-02T20:08:45.481297Z","shell.execute_reply":"2024-04-02T20:08:54.407229Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"Since this is a binary classification problem, it makes sense to use a logistic regression model.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(n_jobs=-1)\nclf.fit(X_train_vectorized, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T16:34:35.271091Z","iopub.execute_input":"2024-04-02T16:34:35.271450Z","iopub.status.idle":"2024-04-02T16:34:38.730629Z","shell.execute_reply.started":"2024-04-02T16:34:35.271421Z","shell.execute_reply":"2024-04-02T16:34:38.729561Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"LogisticRegression(n_jobs=-1)","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(n_jobs=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(n_jobs=-1)</pre></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"code","source":"# Calculate predictions on the test set\nX_test_vectorized = vectorizer.transform(X_test)\ny_pred = clf.predict(X_test_vectorized)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T16:34:38.732520Z","iopub.execute_input":"2024-04-02T16:34:38.732824Z","iopub.status.idle":"2024-04-02T16:34:40.811592Z","shell.execute_reply.started":"2024-04-02T16:34:38.732794Z","shell.execute_reply":"2024-04-02T16:34:40.810724Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, classification_report\n\naccuracy_score(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T16:34:40.812864Z","iopub.execute_input":"2024-04-02T16:34:40.813187Z","iopub.status.idle":"2024-04-02T16:34:40.822350Z","shell.execute_reply.started":"2024-04-02T16:34:40.813158Z","shell.execute_reply":"2024-04-02T16:34:40.821288Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"0.8995"},"metadata":{}}]},{"cell_type":"code","source":"print(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-04-02T16:34:47.330540Z","iopub.execute_input":"2024-04-02T16:34:47.330902Z","iopub.status.idle":"2024-04-02T16:34:47.358395Z","shell.execute_reply.started":"2024-04-02T16:34:47.330873Z","shell.execute_reply":"2024-04-02T16:34:47.357447Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.91      0.88      0.90      5000\n           1       0.89      0.92      0.90      5000\n\n    accuracy                           0.90     10000\n   macro avg       0.90      0.90      0.90     10000\nweighted avg       0.90      0.90      0.90     10000\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"90% accuracy and F-1 score! that's pretty good. But maybe we can improve it. First, let's try stemming.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport random\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\nfrom transformers import AutoModelForSequenceClassification, AdamW, DistilBertTokenizer, get_linear_schedule_with_warmup","metadata":{"execution":{"iopub.status.busy":"2024-04-02T20:10:38.270836Z","iopub.execute_input":"2024-04-02T20:10:38.271725Z","iopub.status.idle":"2024-04-02T20:10:50.663698Z","shell.execute_reply.started":"2024-04-02T20:10:38.271693Z","shell.execute_reply":"2024-04-02T20:10:50.662779Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Use GPU when available\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-04-02T20:10:54.895556Z","iopub.execute_input":"2024-04-02T20:10:54.896510Z","iopub.status.idle":"2024-04-02T20:10:54.931838Z","shell.execute_reply.started":"2024-04-02T20:10:54.896478Z","shell.execute_reply":"2024-04-02T20:10:54.930977Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}]},{"cell_type":"markdown","source":"Tokenize the dataset.","metadata":{}},{"cell_type":"code","source":"# Load the DistilBert tokenizer\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n\ndef tokenize_dataset(reviews):\n    input_ids = []\n    attention_masks = []\n\n    for review in reviews:\n        # `encode_plus` will:\n        #   (1) Tokenize the sentence.\n        #   (2) Prepend the `[CLS]` token to the start.\n        #   (3) Append the `[SEP]` token to the end.\n        #   (4) Map tokens to their IDs.\n        #   (5) Pad or truncate the sentence to `max_length`\n        #   (6) Create attention masks for [PAD] tokens.\n        encoded_dict = tokenizer.encode_plus(\n                            review,                      # Sentence to encode\n                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                            max_length = 512,           # Pad & truncate all sentences\n                            pad_to_max_length = True,\n                            return_attention_mask = True,   # Construct attn. masks\n                            return_tensors = 'pt',     # Return pytorch tensors\n                       )\n\n        # Add the encoded sentence to the list\n        input_ids.append(encoded_dict['input_ids'])\n\n        # And its attention mask (simply differentiates padding from non-padding)\n        attention_masks.append(encoded_dict['attention_mask'])\n\n    return input_ids, attention_masks\n\ninput_ids_train, attention_masks_train = tokenize_dataset(X_train)\ninput_ids_test, attention_masks_test = tokenize_dataset(X_test)\n\n# Convert the lists into tensors\ninput_ids_train = torch.cat(input_ids_train, dim=0)\nattention_masks_train = torch.cat(attention_masks_train, dim=0)\nlabels_train = torch.tensor(y_train.values)\n\ninput_ids_test = torch.cat(input_ids_test, dim=0)\nattention_masks_test = torch.cat(attention_masks_test, dim=0)\nlabels_test = torch.tensor(y_test.values)\n\n# Combine the training inputs into a TensorDataset\ntrain_dataset = TensorDataset(input_ids_train, attention_masks_train, labels_train)\ntest_dataset = TensorDataset(input_ids_test, attention_masks_test, labels_test)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T20:12:13.098877Z","iopub.execute_input":"2024-04-02T20:12:13.099850Z","iopub.status.idle":"2024-04-02T20:19:42.230084Z","shell.execute_reply.started":"2024-04-02T20:12:13.099810Z","shell.execute_reply":"2024-04-02T20:19:42.229274Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b67ec745c0c9430993d46659803a98ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"000066d62e2e4eec8485ec2f734f1394"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02c6bae010b943bba5782745f1861634"}},"metadata":{}},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Create the DataLoaders for our training and test sets.","metadata":{}},{"cell_type":"code","source":"batch_size = 32\n\n# We'll take training samples in random order\ntrain_dataloader = DataLoader(\n            train_dataset,\n            sampler = RandomSampler(train_dataset), # Select batches randomly\n            batch_size = batch_size\n        )\n\n# For testing the order doesn't matter, so we'll just read them sequentially\ntest_dataloader = DataLoader(\n            test_dataset,\n            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially\n            batch_size = batch_size\n        )","metadata":{"execution":{"iopub.status.busy":"2024-04-02T20:19:42.231997Z","iopub.execute_input":"2024-04-02T20:19:42.232732Z","iopub.status.idle":"2024-04-02T20:19:42.238153Z","shell.execute_reply.started":"2024-04-02T20:19:42.232700Z","shell.execute_reply":"2024-04-02T20:19:42.237119Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"Define the model.","metadata":{}},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(\n    \"distilbert-base-uncased\",\n    num_labels = 2,\n    output_attentions = False,\n    output_hidden_states = False\n)\n\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T20:19:42.239390Z","iopub.execute_input":"2024-04-02T20:19:42.239738Z","iopub.status.idle":"2024-04-02T20:19:45.472345Z","shell.execute_reply.started":"2024-04-02T20:19:42.239705Z","shell.execute_reply":"2024-04-02T20:19:45.471301Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d80a71b3754641939999c5c1b493f000"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Define the optimizer.","metadata":{}},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(),\n                  lr = 2e-5,\n                  eps = 1e-8\n                )","metadata":{"execution":{"iopub.status.busy":"2024-04-02T20:19:45.475981Z","iopub.execute_input":"2024-04-02T20:19:45.476286Z","iopub.status.idle":"2024-04-02T20:19:45.484293Z","shell.execute_reply.started":"2024-04-02T20:19:45.476261Z","shell.execute_reply":"2024-04-02T20:19:45.483398Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Define the learning rate scheduler.","metadata":{}},{"cell_type":"code","source":"epochs = 3\n\n# Total number of training steps is [number of batches] x [number of epochs]\ntotal_steps = len(train_dataloader) * epochs\n\n# Create the learning rate scheduler\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0,\n                                            num_training_steps = total_steps)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T20:19:45.485495Z","iopub.execute_input":"2024-04-02T20:19:45.485784Z","iopub.status.idle":"2024-04-02T20:19:45.497190Z","shell.execute_reply.started":"2024-04-02T20:19:45.485761Z","shell.execute_reply":"2024-04-02T20:19:45.496245Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"Define our metric.","metadata":{}},{"cell_type":"code","source":"# Function to calculate the accuracy of our predictions vs labels\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T20:19:45.498222Z","iopub.execute_input":"2024-04-02T20:19:45.498592Z","iopub.status.idle":"2024-04-02T20:19:45.506831Z","shell.execute_reply.started":"2024-04-02T20:19:45.498565Z","shell.execute_reply":"2024-04-02T20:19:45.506099Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"Training loop.","metadata":{}},{"cell_type":"code","source":"seed_val = 42\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\nfor epoch in range(epochs):\n    # Training\n    print('======== Epoch {:} / {:} ========'.format(epoch + 1, epochs))\n    total_train_loss = 0\n    model.train()\n\n    for batch in tqdm(train_dataloader, desc=\"Training\"):\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n        optimizer.zero_grad()\n        output = model(b_input_ids,\n                         attention_mask=b_input_mask, \n                         labels=b_labels)        \n        loss = output.loss\n        total_train_loss += loss.item()\n\n        # Perform a backward pass to calculate the gradients\n        loss.backward()\n\n        # Clip the norm of the gradients to 1.0 to help prevent the \"exploding gradients\" problem\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        # Update parameters and take a step using the computed gradient\n        optimizer.step()\n\n        # Update the learning rate\n        scheduler.step()\n\n    # Calculate the average loss over all of the batches\n    avg_train_loss = total_train_loss / len(train_dataloader)            \n    \n    print(\"\")\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n\n    # Evaluation\n    print(\"\")\n    # Put the model in evaluation mode\n    model.eval()\n\n    total_test_accuracy = 0\n    best_test_accuracy = 0\n    total_test_loss = 0\n\n    for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n\n        # Constructing the compute graph is only needed for backprop (training)\n        with torch.no_grad():        \n            output = model(b_input_ids,\n                           attention_mask=b_input_mask,\n                           labels=b_labels)\n        loss = output.loss\n        total_test_loss += loss.item()\n\n        # Move logits and labels to CPU if we are using GPU\n        logits = output.logits\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n\n        # Calculate the accuracy for this batch of test reviews, and\n        # accumulate it over all batches\n        total_test_accuracy += flat_accuracy(logits, label_ids)\n\n    # Report the final accuracy for this validation run\n    avg_test_accuracy = total_test_accuracy / len(test_dataloader)\n    print(\"  Accuracy: {0:.3f}\".format(avg_test_accuracy))\n    print(\"\")\n\n    # Calculate the average loss over all of the batches\n    avg_val_loss = total_test_loss / len(test_dataloader)\n\n    # Save the best model\n    if avg_test_accuracy > best_test_accuracy:\n        torch.save(model, 'bert_model.pt')\n        best_test_accuracy = avg_test_accuracy\n\nprint(\"\")\nprint(\"Training complete!\")","metadata":{"execution":{"iopub.status.busy":"2024-04-02T20:19:45.508167Z","iopub.execute_input":"2024-04-02T20:19:45.508422Z","iopub.status.idle":"2024-04-02T21:16:19.202790Z","shell.execute_reply.started":"2024-04-02T20:19:45.508400Z","shell.execute_reply":"2024-04-02T21:16:19.201372Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"======== Epoch 1 / 3 ========\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1250/1250 [17:21<00:00,  1.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n  Average training loss: 0.24\n\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 313/313 [01:28<00:00,  3.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Accuracy: 0.935\n\n======== Epoch 2 / 3 ========\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1250/1250 [17:22<00:00,  1.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n  Average training loss: 0.13\n\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 313/313 [01:28<00:00,  3.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Accuracy: 0.941\n\n======== Epoch 3 / 3 ========\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1250/1250 [17:22<00:00,  1.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n  Average training loss: 0.08\n\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 313/313 [01:28<00:00,  3.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Accuracy: 0.942\n\n\nTraining complete!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let's calculate predictions on the entire test set.","metadata":{}},{"cell_type":"code","source":"predictions = []\n\nfor batch in test_dataloader:\n    b_input_ids = batch[0].to(device)\n    b_input_mask = batch[1].to(device)\n    b_labels = batch[2].to(device)\n\n    with torch.no_grad():        \n        output = model(b_input_ids,\n                       attention_mask=b_input_mask,\n                       labels=b_labels)\n\n    logits = output.logits\n    logits = logits.detach().cpu().numpy()\n    label_ids = b_labels.to('cpu').numpy()\n\n    preds = np.argmax(logits, axis=1).flatten()\n    predictions.append(preds)\n\npredictions = np.concatenate(predictions)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T21:16:19.204717Z","iopub.execute_input":"2024-04-02T21:16:19.205207Z","iopub.status.idle":"2024-04-02T21:17:47.089922Z","shell.execute_reply.started":"2024-04-02T21:16:19.205163Z","shell.execute_reply":"2024-04-02T21:17:47.089017Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, classification_report\naccuracy_score(y_test, predictions)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T21:30:45.564240Z","iopub.execute_input":"2024-04-02T21:30:45.564601Z","iopub.status.idle":"2024-04-02T21:30:45.573357Z","shell.execute_reply.started":"2024-04-02T21:30:45.564573Z","shell.execute_reply":"2024-04-02T21:30:45.572471Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"0.9415"},"metadata":{}}]},{"cell_type":"code","source":"print(classification_report(y_test, predictions))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-02T21:30:48.393944Z","iopub.execute_input":"2024-04-02T21:30:48.394915Z","iopub.status.idle":"2024-04-02T21:30:48.421891Z","shell.execute_reply.started":"2024-04-02T21:30:48.394884Z","shell.execute_reply":"2024-04-02T21:30:48.421024Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.94      0.94      0.94      5000\n           1       0.94      0.94      0.94      5000\n\n    accuracy                           0.94     10000\n   macro avg       0.94      0.94      0.94     10000\nweighted avg       0.94      0.94      0.94     10000\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}